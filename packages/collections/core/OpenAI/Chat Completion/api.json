{
  "created_at": "2023-08-30T08:24:48.110773+00:00",
  "title": "Chat Completion",
  "summary": "Creates a model response for the given chat conversation.",
  "method": "POST",
  "path": "https://api.openai.com/v1/chat/completions",
  "project": "OpenAI",
  "auth": "bearer",
  "tags": [
    "Popular",
    "Main",
    "Examples"
  ],
  "queryParams": null,
  "urlParams": null,
  "requestBody": {
    "$schema": "http://json-schema.org/draft-07/schema#",
    "type": "object",
    "properties": {
      "model": {
        "type": "string",
        "description": "ID of the model to use. See the [model endpoint compatibility](https://platform.openai.com/docs/models/model-endpoint-compatibility) table for details on which models work with the Chat API.",
        "default": "gpt-3.5-turbo"
      },
      "messages": {
        "type": "array",
        "items": {
          "type": "object",
          "properties": {
            "role": {
              "type": "string",
              "enum": [
                "system",
                "user",
                "assistant"
              ],
              "description": "The role of the messages author. One of `system`, `user`, `assistant`.",
              "default": "system"
            },
            "content": {
              "type": "string",
              "description": "The contents of the message. `content` is required for all messages, and may be null for assistant messages with function calls."
            }
          },
          "required": [
            "role",
            "content"
          ],
          "additionalProperties": false
        },
        "description": "A list of messages comprising the conversation so far. [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.ipynb)."
      },
      "temperature": {
        "type": "number",
        "description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\\n\\nWe generally recommend altering this or `top_p` but not both.\\n",
        "maximum": 2,
        "default": 1
      },
      "top_p": {
        "type": "number",
        "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\\n\\nWe generally recommend altering this or `temperature` but not both.\\n",
        "minimum": 1,
        "maximum": 1
      },
      "n": {
        "type": "number",
        "description": "How many chat completion choices to generate for each input message.",
        "minimum": 1,
        "maximum": 128,
        "default": 1
      },
      "stream": {
        "type": "boolean",
        "description": "Whether to stream the output. Streaming output is good for frontend UX but not needed in backend usually."
      },
      "stop": {
        "anyOf": [
          {
            "type": "string"
          },
          {
            "type": "array",
            "items": {
              "type": "string"
            }
          }
        ],
        "description": "Up to 4 sequences where the API will stop generating further tokens."
      },
      "max_tokens": {
        "type": "number",
        "description": "The maximum number of [tokens](https://platform.openai.com/tokenizer) to generate in the chat completion.\\n\\nThe total length of input tokens and generated tokens is limited by the model's context length. [Example Python code](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb) for counting tokens."
      },
      "presence_penalty": {
        "type": "number",
        "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\\n\\n[See more information about frequency and presence penalties.](https://platform.openai.com/docs/api-reference/parameter-details)",
        "minimum": -2,
        "maximum": 2
      },
      "frequency_penalty": {
        "type": "number",
        "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\\n\\n[See more information about frequency and presence penalties.](https://platform.openai.com/docs/api-reference/parameter-details)",
        "minimum": -2,
        "maximum": 2
      },
      "logit_bias": {
        "type": "object",
        "additionalProperties": {
          "type": "number"
        },
        "description": "Modify the likelihood of specified tokens appearing in the completion.\\n\\nAccepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token."
      },
      "user": {
        "type": "string",
        "description": "A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids)."
      }
    },
    "required": [
      "model",
      "messages"
    ],
    "additionalProperties": false
  },
  "options": {
    "docs": {
      "auth": "https://docs.recipeui.com/docs/Auth/openai"
    }
  },
  "rank": 1,
  "author_id": null,
  "visibility": "public",
  "version": 1,
  "id": "48f37734-bbf4-4d0e-81b4-08da77030b06",
  "queryParamsType": "",
  "urlParamsType": null,
  "requestBodyType": "export interface APIRequestParams {\n    model:    \"gpt-4\" | \"gpt-3.5-turbo\" | string;\n    messages: Message[];\n    temperature?: number;\n    top_p?: number;\n    n?: number;\n    stream?: boolean;\n    stop?: string | string[];\n    max_tokens?: number;\n    presence_penalty?: number;\n    frequency_penalty?: number;\n    logit_bias?: Record<string, number>;\n    user?: string;\n}\n\ninterface Message {\n    role:    \"system\" | \"user\" | \"assistant\";\n    content: string;\n}"
}